use once_cell::sync::OnceCell;
use serde::{Deserialize, Serialize};
// use blake2::{Blake2s256, Digest}; // Removed
use blake3;
use std::collections::HashMap;
use std::fs::{self, File};
use std::io::{BufReader, BufWriter, Read, Write};
use std::path::PathBuf;
use std::sync::Arc;
use tauri::Emitter;
use tokio::sync::RwLock;
use uuid::Uuid;

const NUM_CHUNKS: usize = 10;

// ============================================
// GLOBAL CHUNK INDEX
// ============================================

/// Global in-memory index: chunk_hash -> PathBuf
static CHUNK_INDEX: OnceCell<Arc<RwLock<HashMap<String, PathBuf>>>> = OnceCell::new();

fn get_or_init_index() -> &'static Arc<RwLock<HashMap<String, PathBuf>>> {
    CHUNK_INDEX.get_or_init(|| Arc::new(RwLock::new(HashMap::new())))
}

/// Initialize the chunk index by scanning the chunks directory once at startup.
/// Walks both flat `{hash}.chunk` files and legacy `{file_id}/chunk_XXX` subdirectories.
pub async fn init_chunk_index(chunks_dir: &PathBuf) {
    let index = get_or_init_index();
    let mut map = index.write().await;
    map.clear();

    let entries = match fs::read_dir(chunks_dir) {
        Ok(e) => e,
        Err(e) => {
            println!("[ChunkIndex] Failed to read chunks dir: {}", e);
            return;
        }
    };

    for entry in entries.flatten() {
        let path = entry.path();
        if path.is_file() {
            // Flat hash-named file: {hash}.chunk
            if let Some(stem) = path.file_stem().and_then(|s| s.to_str()) {
                if path.extension().map_or(false, |ext| ext == "chunk") {
                    map.insert(stem.to_string(), path);
                    continue;
                }
            }

            // Compute hash if not a .chunk file
            if let Ok(hash) = calculate_chunk_hash_internal(&path) {
                map.insert(hash, path);
            }
        } else if path.is_dir() {
            // Legacy subdirectory: {file_id}/chunk_XXX
            if let Ok(sub_entries) = fs::read_dir(&path) {
                for sub_entry in sub_entries.flatten() {
                    let sub_path = sub_entry.path();
                    if sub_path.is_file() {
                        if let Ok(hash) = calculate_chunk_hash_internal(&sub_path) {
                            map.insert(hash, sub_path);
                        }
                    }
                }
            }
        }
    }

    println!("[ChunkIndex] Initialized with {} entries", map.len());
}

/// ✅ ASYNC ONLY — O(1) lookup
pub async fn get_chunk_path_by_hash(hash: &str) -> Option<PathBuf> {
    let index = get_or_init_index();
    let map = index.read().await;
    map.get(hash).cloned()
}

/// Insert a chunk into the index after creation or storage.
pub async fn insert_chunk_index(hash: String, path: PathBuf) {
    let index = get_or_init_index();
    let mut map = index.write().await;
    map.insert(hash, path);
}

/// Remove a chunk from the index after deletion.
pub async fn remove_chunk_index(hash: &str) {
    let index = get_or_init_index();
    let mut map = index.write().await;
    map.remove(hash);
}

/// Metadata about a chunked file
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct ChunkedFile {
    pub id: String,
    pub original_name: String,
    pub total_size: u64,
    pub chunk_count: usize,
    pub chunk_size: u64,
    pub chunks_dir: String,
    #[serde(default)]
    pub backend_file_id: Option<String>,
    #[serde(default)]
    pub folder_id: Option<String>,
}

/// Info about a single chunk
#[derive(Debug, Serialize, Deserialize)]
pub struct ChunkInfo {
    pub chunk_index: usize,
    pub chunk_path: String,
    pub chunk_size: u64,
}

/// Metadata about a folder
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct Folder {
    pub id: String,
    pub name: String,
    pub created_at: String,
}

/// Get the chunks storage directory
fn get_chunks_dir() -> Result<PathBuf, String> {
    let data_dir =
        dirs::data_local_dir().ok_or_else(|| "Could not find local data directory".to_string())?;

    let chunks_dir = data_dir.join("meshdrive").join("chunks");

    if !chunks_dir.exists() {
        fs::create_dir_all(&chunks_dir)
            .map_err(|e| format!("Failed to create chunks directory: {}", e))?;
    }

    Ok(chunks_dir)
}

/// Get the metadata directory for storing chunk info
fn get_metadata_dir() -> Result<PathBuf, String> {
    let data_dir =
        dirs::data_local_dir().ok_or_else(|| "Could not find local data directory".to_string())?;

    let meta_dir = data_dir.join("meshdrive").join("metadata");

    if !meta_dir.exists() {
        fs::create_dir_all(&meta_dir)
            .map_err(|e| format!("Failed to create metadata directory: {}", e))?;
    }

    Ok(meta_dir)
}

/// Get the folders metadata directory
fn get_folders_dir() -> Result<PathBuf, String> {
    let meta_dir = get_metadata_dir()?;
    let folders_dir = meta_dir.join("folders");

    if !folders_dir.exists() {
        fs::create_dir_all(&folders_dir)
            .map_err(|e| format!("Failed to create folders directory: {}", e))?;
    }

    Ok(folders_dir)
}

/// Upload and chunk a file into NUM_CHUNKS pieces
#[tauri::command]
pub async fn upload_and_chunk_file(source_path: String) -> Result<ChunkedFile, String> {
    let source = PathBuf::from(&source_path);

    // Validate source file exists
    if !source.exists() {
        return Err(format!("Source file does not exist: {}", source_path));
    }

    // Get original filename
    let original_name = source
        .file_name()
        .and_then(|n| n.to_str())
        .ok_or_else(|| "Invalid filename".to_string())?
        .to_string();

    // Get file size
    let metadata =
        fs::metadata(&source).map_err(|e| format!("Failed to get file metadata: {}", e))?;
    let total_size = metadata.len();

    // Generate unique ID for this file
    let file_id = Uuid::new_v4().to_string();

    // Create directory for this file's chunks
    let chunks_base_dir = get_chunks_dir()?;
    let file_chunks_dir = chunks_base_dir.join(&file_id);
    fs::create_dir_all(&file_chunks_dir)
        .map_err(|e| format!("Failed to create file chunks directory: {}", e))?;

    // Calculate chunk size (divide evenly, last chunk may be smaller)
    let chunk_size = (total_size as f64 / NUM_CHUNKS as f64).ceil() as u64;

    // Open source file for reading
    let file = File::open(&source).map_err(|e| format!("Failed to open source file: {}", e))?;
    let mut reader = BufReader::new(file);

    // Split into chunks
    let mut buffer = vec![0u8; chunk_size as usize];
    let mut chunk_index = 0;

    loop {
        let bytes_read = reader
            .read(&mut buffer)
            .map_err(|e| format!("Failed to read from source file: {}", e))?;

        if bytes_read == 0 {
            break; // EOF
        }

        // Write chunk to file
        let chunk_filename = format!("chunk_{:03}", chunk_index);
        let chunk_path = file_chunks_dir.join(&chunk_filename);

        let chunk_file =
            File::create(&chunk_path).map_err(|e| format!("Failed to create chunk file: {}", e))?;
        let mut writer = BufWriter::new(chunk_file);

        writer
            .write_all(&buffer[..bytes_read])
            .map_err(|e| format!("Failed to write chunk: {}", e))?;

        // Insert into chunk index
        let mut hasher = blake3::Hasher::new();
        hasher.update(&buffer[..bytes_read]);
        let hash = hasher.finalize().to_hex().to_string();
        insert_chunk_index(hash, chunk_path).await;

        chunk_index += 1;
    }

    // Create metadata
    let chunked_file = ChunkedFile {
        id: file_id.clone(),
        original_name,
        total_size,
        chunk_count: chunk_index,
        chunk_size,
        chunks_dir: file_chunks_dir.to_string_lossy().to_string(),
        backend_file_id: None,
        folder_id: None,
    };

    // Save metadata to JSON file
    let metadata_dir = get_metadata_dir()?;
    let metadata_path = metadata_dir.join(format!("{}.json", file_id));
    let metadata_json = serde_json::to_string_pretty(&chunked_file)
        .map_err(|e| format!("Failed to serialize metadata: {}", e))?;

    fs::write(&metadata_path, metadata_json)
        .map_err(|e| format!("Failed to write metadata: {}", e))?;

    Ok(chunked_file)
}

/// List all chunked files
#[tauri::command]
pub async fn list_chunked_files() -> Result<Vec<ChunkedFile>, String> {
    let metadata_dir = get_metadata_dir()?;
    let mut files = Vec::new();

    let entries = fs::read_dir(&metadata_dir)
        .map_err(|e| format!("Failed to read metadata directory: {}", e))?;

    for entry in entries {
        let entry = entry.map_err(|e| format!("Failed to read entry: {}", e))?;
        let path = entry.path();

        if path.extension().map_or(false, |ext| ext == "json") {
            let content = fs::read_to_string(&path)
                .map_err(|e| format!("Failed to read metadata file: {}", e))?;

            let chunked_file: ChunkedFile = serde_json::from_str(&content)
                .map_err(|e| format!("Failed to parse metadata: {}", e))?;

            files.push(chunked_file);
        }
    }

    Ok(files)
}

/// Get chunks info for a specific file
#[tauri::command]
pub async fn get_file_chunks(file_id: String) -> Result<Vec<ChunkInfo>, String> {
    let metadata_dir = get_metadata_dir()?;
    let metadata_path = metadata_dir.join(format!("{}.json", file_id));

    let content = fs::read_to_string(&metadata_path)
        .map_err(|e| format!("Failed to read metadata: {}", e))?;

    let chunked_file: ChunkedFile =
        serde_json::from_str(&content).map_err(|e| format!("Failed to parse metadata: {}", e))?;

    let chunks_dir = PathBuf::from(&chunked_file.chunks_dir);
    let mut chunks = Vec::new();

    for i in 0..chunked_file.chunk_count {
        let chunk_filename = format!("chunk_{:03}", i);
        let chunk_path = chunks_dir.join(&chunk_filename);

        let chunk_metadata = fs::metadata(&chunk_path)
            .map_err(|e| format!("Failed to get chunk metadata: {}", e))?;

        chunks.push(ChunkInfo {
            chunk_index: i,
            chunk_path: chunk_path.to_string_lossy().to_string(),
            chunk_size: chunk_metadata.len(),
        });
    }

    Ok(chunks)
}

/// Merge chunks back into original file and save to specified location
#[tauri::command]
pub async fn download_and_merge_file(
    file_id: String,
    destination_path: String,
) -> Result<String, String> {
    let metadata_dir = get_metadata_dir()?;
    let metadata_path = metadata_dir.join(format!("{}.json", file_id));

    let content = fs::read_to_string(&metadata_path)
        .map_err(|e| format!("Failed to read metadata: {}", e))?;

    let chunked_file: ChunkedFile =
        serde_json::from_str(&content).map_err(|e| format!("Failed to parse metadata: {}", e))?;

    let chunks_dir = PathBuf::from(&chunked_file.chunks_dir);

    // Determine output path
    let output_path = if destination_path.is_empty() {
        // Default to Downloads folder
        let downloads =
            dirs::download_dir().ok_or_else(|| "Could not find downloads directory".to_string())?;
        downloads.join(&chunked_file.original_name)
    } else {
        PathBuf::from(&destination_path).join(&chunked_file.original_name)
    };

    // Create output file
    let output_file =
        File::create(&output_path).map_err(|e| format!("Failed to create output file: {}", e))?;
    let mut writer = BufWriter::new(output_file);

    // Read and write each chunk in order
    for i in 0..chunked_file.chunk_count {
        let chunk_filename = format!("chunk_{:03}", i);
        let chunk_path = chunks_dir.join(&chunk_filename);

        let mut chunk_data = Vec::new();
        let chunk_file =
            File::open(&chunk_path).map_err(|e| format!("Failed to open chunk {}: {}", i, e))?;
        let mut reader = BufReader::new(chunk_file);

        reader
            .read_to_end(&mut chunk_data)
            .map_err(|e| format!("Failed to read chunk {}: {}", i, e))?;

        writer
            .write_all(&chunk_data)
            .map_err(|e| format!("Failed to write chunk {}: {}", i, e))?;
    }

    writer
        .flush()
        .map_err(|e| format!("Failed to flush output file: {}", e))?;

    Ok(output_path.to_string_lossy().to_string())
}

/// Helper to delete local chunks and metadata only (no backend/p2p calls)
pub fn delete_local_chunks_only(file_id: String) -> Result<(), String> {
    let metadata_dir = get_metadata_dir()?;
    let metadata_path = metadata_dir.join(format!("{}.json", file_id));

    // Check if metadata exists
    if !metadata_path.exists() {
        // If metadata is gone, check if chunks folder exists anyway and delete it (cleanup orphan)
        let chunks_base_dir = get_chunks_dir()?;
        let potential_chunks_dir = chunks_base_dir.join(&file_id);

        if potential_chunks_dir.exists() {
            // Remove index entries for orphan chunks before deleting
            if let Ok(entries) = fs::read_dir(&potential_chunks_dir) {
                for entry in entries.flatten() {
                    let path = entry.path();
                    if path.is_file() {
                        if let Ok(hash) = calculate_chunk_hash_internal(&path) {
                            let hash_clone = hash.clone();
                            tokio::spawn(async move {
                                remove_chunk_index(&hash_clone).await;
                            });
                        }
                    }
                }
            }

            fs::remove_dir_all(&potential_chunks_dir)
                .map_err(|e| format!("Failed to delete orphan chunks: {}", e))?;
        }

        return Ok(());
    }

    // Read metadata to get chunks directory
    let content = fs::read_to_string(&metadata_path)
        .map_err(|e| format!("Failed to read metadata: {}", e))?;

    let chunked_file: ChunkedFile =
        serde_json::from_str(&content).map_err(|e| format!("Failed to parse metadata: {}", e))?;

    // Delete chunks directory and remove from index
    let chunks_dir = PathBuf::from(&chunked_file.chunks_dir);

    if chunks_dir.exists() {
        // Remove index entries before deleting files
        if let Ok(entries) = fs::read_dir(&chunks_dir) {
            for entry in entries.flatten() {
                let path = entry.path();
                if path.is_file() {
                    if let Ok(hash) = calculate_chunk_hash_internal(&path) {
                        let hash_clone = hash.clone();
                        tokio::spawn(async move {
                            remove_chunk_index(&hash_clone).await;
                        });
                    }
                }
            }
        }

        fs::remove_dir_all(&chunks_dir).map_err(|e| format!("Failed to delete chunks: {}", e))?;
    }

    // Delete metadata file
    fs::remove_file(&metadata_path).map_err(|e| format!("Failed to delete metadata: {}", e))?;

    Ok(())
}

/// Delete a chunked file and all its chunks (Local + Backend + P2P)
#[tauri::command]
pub async fn delete_chunked_file(file_id: String, app: tauri::AppHandle) -> Result<(), String> {
    println!("[Delete] Starting deletion for file: {}", file_id);

    // 1. Get metadata to find backend ID
    let metadata_dir = get_metadata_dir()?;
    let metadata_path = metadata_dir.join(format!("{}.json", file_id));

    // Initial read to get backend ID
    let backend_file_id = if metadata_path.exists() {
        match fs::read_to_string(&metadata_path) {
            Ok(content) => match serde_json::from_str::<ChunkedFile>(&content) {
                Ok(meta) => meta.backend_file_id,
                Err(_) => None,
            },
            Err(_) => None,
        }
    } else {
        None
    };

    // Prepare to collect hashes
    let mut all_chunk_hashes = Vec::new();
    let mut token = String::new();

    // 2. Try to fetch FULL hash list from backend first
    if let Some(bid) = &backend_file_id {
        if let Ok(auth_state) = crate::auth::load_auth_state() {
            token = auth_state.token.clone();
            if !token.is_empty() {
                let api = crate::api_client::ApiClient::new();
                println!("[Delete] Fetching full chunk list from backend...");
                match api.get_file_download_info(&token, bid).await {
                    Ok(info) => {
                        all_chunk_hashes = info
                            .chunks
                            .iter()
                            .map(|c| c.chunk.chunk_hash.clone())
                            .collect();
                        println!(
                            "[Delete] ✓ Retrieved {} hashes from backend",
                            all_chunk_hashes.len()
                        );
                    }
                    Err(e) => {
                        println!("[Delete] Failed to fetch info from backend: {}", e);
                    }
                }
            }
        }
    }

    // 3. Fallback: Calculate from local files if backend failed or returned empty
    if all_chunk_hashes.is_empty() && metadata_path.exists() {
        println!("[Delete] Fallback: Calculating hashes from local files...");
        if let Ok(content) = fs::read_to_string(&metadata_path) {
            if let Ok(meta) = serde_json::from_str::<ChunkedFile>(&content) {
                let chunks_dir = PathBuf::from(&meta.chunks_dir);
                for i in 0..meta.chunk_count {
                    let chunk_filename = format!("chunk_{:03}", i);
                    let chunk_path = chunks_dir.join(&chunk_filename);
                    if let Ok(mut file) = File::open(&chunk_path) {
                        let mut hasher = blake3::Hasher::new();
                        let mut buffer = [0; 64 * 1024];
                        loop {
                            match file.read(&mut buffer) {
                                Ok(0) => break,
                                Ok(n) => { hasher.update(&buffer[..n]); },
                                Err(_) => break,
                            }
                        }
                        all_chunk_hashes.push(hasher.finalize().to_hex().to_string());
                    }
                }
            }
        }
        println!(
            "[Delete] ✓ Calculated {} hashes locally",
            all_chunk_hashes.len()
        );
    }

    // 4. Delete from Backend (if registered)
    if let Some(bid) = &backend_file_id {
        println!("[Delete] Removing from backend: {}", bid);

        if !token.is_empty() {
            let api = crate::api_client::ApiClient::new();
            if let Err(e) = api.delete_file(&token, bid).await {
                println!("[Delete] Warning: Failed to delete from backend: {}", e);
                // We continue execution to ensure local cleanup
            } else {
                println!("[Delete] ✓ Removed from backend");
            }
        }
    }

    // 5. Broadcast Delete to Peers & Save for Offline Peers
    if !all_chunk_hashes.is_empty() {
        // Save to pending deletes FIRST ensuring it persists even if broadcast fails
        if let Err(e) = add_pending_delete(file_id.clone(), all_chunk_hashes.clone()) {
            println!("[Delete] Warning: Failed to save pending delete: {}", e);
        }

        if let Some(node) = crate::p2p::get_p2p_node() {
            println!(
                "[Delete] Broadcasting delete to peers with {} hashes...",
                all_chunk_hashes.len()
            );
            let count = node
                .broadcast_delete(file_id.clone(), all_chunk_hashes)
                .await;
            println!("[Delete] ✓ Notified {} peers", count);
        }
    } else {
        println!("[Delete] Warning: No hashes, skipping peer broadcast and pending save");
    }

    // 4. Delete Local Data
    println!("[Delete] Removing local data...");
    delete_local_chunks_only(file_id)?;
    println!("[Delete] ✓ Local data removed");

    Ok(())
}

/// Get the chunks directory path
#[tauri::command]
pub fn get_chunks_directory() -> Result<String, String> {
    let chunks_dir = get_chunks_dir()?;
    Ok(chunks_dir.to_string_lossy().to_string())
}

/// ✅ NEW: Calculate hash for each chunk

pub fn calculate_chunk_hash_internal(chunk_path: &PathBuf) -> Result<String, String> {
    let mut file = File::open(chunk_path).map_err(|e| format!("Failed to open chunk: {}", e))?;

    let mut hasher = blake3::Hasher::new();
    let mut buffer = Vec::new();

    file.read_to_end(&mut buffer)
        .map_err(|e| format!("Failed to read chunk: {}", e))?;

    hasher.update(&buffer);
    let hash = hasher.finalize();

    Ok(hash.to_hex().to_string())
}

/// Upload, chunk, and register with progress events
#[tauri::command]
pub async fn upload_chunk_and_register(
    source_path: String,
    folder_id: Option<String>,
    app: tauri::AppHandle,
) -> Result<String, String> {
    println!("[Upload] Starting complete upload flow...");

    // Helper function to emit progress
    let emit_progress = |stage: &str, progress: u8| {
        let _ = app.emit(
            "upload-progress",
            serde_json::json!({
                "stage": stage,
                "progress": progress
            }),
        );
    };

    // 1. Chunk the file
    emit_progress("Chunking file...", 10);
    println!("[Upload] Step 1/5: Chunking file...");
    let chunked_file = upload_and_chunk_file(source_path.clone()).await?;
    println!("[Upload] ✓ Created {} chunks", chunked_file.chunk_count);
    emit_progress("Chunking complete", 20);

    // 2. Calculate hash for each chunk
    emit_progress("Calculating hashes...", 25);
    println!("[Upload] Step 2/5: Calculating chunk hashes...");
    let chunks_dir = PathBuf::from(&chunked_file.chunks_dir);
    let mut chunk_hashes = Vec::new();

    for i in 0..chunked_file.chunk_count {
        let chunk_filename = format!("chunk_{:03}", i);
        let chunk_path = chunks_dir.join(&chunk_filename);

        let chunk_hash = calculate_chunk_hash_internal(&chunk_path)?;
        chunk_hashes.push(chunk_hash);

        // Update progress for each chunk hash calculated
        let hash_progress = 25 + ((i + 1) as f32 / chunked_file.chunk_count as f32 * 15.0) as u8;
        emit_progress(
            &format!("Hashing chunk {}/{}", i + 1, chunked_file.chunk_count),
            hash_progress,
        );
    }
    println!("[Upload] ✓ Calculated {} hashes", chunk_hashes.len());
    emit_progress("Hashing complete", 40);

    // 3. Upload file metadata to backend
    emit_progress("Registering with backend...", 45);
    println!("[Upload] Step 3/5: Registering file with backend...");
    let uploaded =
        crate::upload::upload_file_with_backend(source_path, chunked_file.chunk_count as i32)
            .await?;

    let backend_file_id = uploaded
        .backend_file_id
        .ok_or_else(|| "Failed to get backend file ID".to_string())?;

    println!("[Upload] ✓ File registered: {}", backend_file_id);
    emit_progress("Backend registration complete", 60);

    // 4. Register chunks with backend
    emit_progress("Registering chunks...", 65);
    println!("[Upload] Step 4/5: Registering chunks with backend...");
    let _chunk_ids = crate::upload::register_chunks_with_backend(
        backend_file_id.clone(),
        chunk_hashes.clone(),
        chunked_file.chunk_size as i32,
    )
    .await?;
    println!("[Upload] ✓ Chunks registered");
    emit_progress("Chunks registered", 75);

    // 5. Save backend_file_id to local metadata
    emit_progress("Updating metadata...", 80);
    let metadata_dir = get_metadata_dir()?;
    let metadata_path = metadata_dir.join(format!("{}.json", chunked_file.id));
    let mut updated_meta = chunked_file.clone();
    updated_meta.backend_file_id = Some(backend_file_id.clone());
    updated_meta.folder_id = folder_id;
    let metadata_json = serde_json::to_string_pretty(&updated_meta)
        .map_err(|e| format!("Failed to serialize metadata: {}", e))?;
    fs::write(&metadata_path, metadata_json)
        .map_err(|e| format!("Failed to update metadata: {}", e))?;
    emit_progress("Metadata updated", 85);

    // 6. AUTO-DISTRIBUTE chunks to peers
    emit_progress("Distributing to peers...", 90);
    println!("[Upload] Step 5/5: Distributing chunks to peers...");
    match crate::download::distribute_chunks_to_peers(
        chunked_file.id.clone(),
        backend_file_id.clone(),
    )
    .await
    {
        Ok(result) => {
            println!("[Upload] ✓ Distribution complete!");
            println!(
                "[Upload]   - Chunks distributed: {}",
                result.chunks_distributed
            );
            println!("[Upload]   - Peers available: {}", result.peers_reached);
            if !result.errors.is_empty() {
                println!("[Upload]   - Warnings: {:?}", result.errors);
            }
            emit_progress("Distribution complete", 95);
        }
        Err(e) => {
            println!("[Upload] ⚠ Distribution warning: {}", e);
            emit_progress("Distribution warning", 95);
            // Don't fail the upload if distribution fails
        }
    }

    emit_progress("Upload complete!", 100);
    println!("[Upload] ✅ Upload complete! File ID: {}", backend_file_id);

    Ok(backend_file_id)
}

// ============================================
// FOLDER MANAGEMENT COMMANDS
// ============================================

/// Create a new folder
#[tauri::command]
pub async fn create_folder(name: String) -> Result<Folder, String> {
    let folder_id = Uuid::new_v4().to_string();
    let now = chrono::Local::now().to_rfc3339();

    let folder = Folder {
        id: folder_id.clone(),
        name,
        created_at: now,
    };

    let folders_dir = get_folders_dir()?;
    let folder_path = folders_dir.join(format!("{}.json", folder_id));
    let json = serde_json::to_string_pretty(&folder)
        .map_err(|e| format!("Failed to serialize folder: {}", e))?;
    fs::write(&folder_path, json)
        .map_err(|e| format!("Failed to write folder metadata: {}", e))?;

    println!("[Folders] ✓ Created folder: {} ({})", folder.name, folder.id);
    Ok(folder)
}

/// List all folders
#[tauri::command]
pub async fn list_folders() -> Result<Vec<Folder>, String> {
    let folders_dir = get_folders_dir()?;
    let mut folders = Vec::new();

    let entries = fs::read_dir(&folders_dir)
        .map_err(|e| format!("Failed to read folders directory: {}", e))?;

    for entry in entries {
        let entry = entry.map_err(|e| format!("Failed to read entry: {}", e))?;
        let path = entry.path();

        if path.extension().map_or(false, |ext| ext == "json") {
            let content = fs::read_to_string(&path)
                .map_err(|e| format!("Failed to read folder file: {}", e))?;
            if let Ok(folder) = serde_json::from_str::<Folder>(&content) {
                folders.push(folder);
            }
        }
    }

    // Sort by created_at (oldest first)
    folders.sort_by(|a, b| a.created_at.cmp(&b.created_at));
    Ok(folders)
}

/// Rename a folder
#[tauri::command]
pub async fn rename_folder(folder_id: String, new_name: String) -> Result<Folder, String> {
    let folders_dir = get_folders_dir()?;
    let folder_path = folders_dir.join(format!("{}.json", folder_id));

    if !folder_path.exists() {
        return Err(format!("Folder not found: {}", folder_id));
    }

    let content = fs::read_to_string(&folder_path)
        .map_err(|e| format!("Failed to read folder: {}", e))?;
    let mut folder: Folder = serde_json::from_str(&content)
        .map_err(|e| format!("Failed to parse folder: {}", e))?;

    folder.name = new_name.clone();

    let json = serde_json::to_string_pretty(&folder)
        .map_err(|e| format!("Failed to serialize folder: {}", e))?;
    fs::write(&folder_path, json)
        .map_err(|e| format!("Failed to write folder: {}", e))?;

    println!("[Folders] ✓ Renamed folder {} to '{}'", folder_id, new_name);
    Ok(folder)
}


// ============================================
// OFFLINE DELETION SUPPORT
// ============================================

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct PendingDelete {
    pub file_id: String,
    pub chunk_hashes: Vec<String>,
    pub timestamp: String,
}

fn get_pending_deletes_path() -> Result<PathBuf, String> {
    let meta_dir = get_metadata_dir()?;
    Ok(meta_dir.join("pending_deletes.json"))
}

/// Save a pending deletion to disk
pub fn add_pending_delete(file_id: String, chunk_hashes: Vec<String>) -> Result<(), String> {
    let path = get_pending_deletes_path()?;
    let mut pending: Vec<PendingDelete> = if path.exists() {
        let content = fs::read_to_string(&path)
            .map_err(|e| format!("Failed to read pending deletes: {}", e))?;
        serde_json::from_str(&content).unwrap_or_default()
    } else {
        Vec::new()
    };

    // Check if already exists to avoid duplicates
    if !pending.iter().any(|d| d.file_id == file_id) {
        pending.push(PendingDelete {
            file_id,
            chunk_hashes,
            timestamp: chrono::Local::now().to_rfc3339(),
        });

        let json = serde_json::to_string_pretty(&pending)
            .map_err(|e| format!("Failed to serialize pending deletes: {}", e))?;
        fs::write(&path, json)
            .map_err(|e| format!("Failed to write pending deletes: {}", e))?;
            
        println!("[Delete] ✓ Added file to pending deletes list");
    }

    Ok(())
}

/// Get all pending deletions
pub fn get_pending_deletes() -> Result<Vec<PendingDelete>, String> {
    let path = get_pending_deletes_path()?;
    if !path.exists() {
        return Ok(Vec::new());
    }

    let content = fs::read_to_string(&path)
        .map_err(|e| format!("Failed to read pending deletes: {}", e))?;
    let pending: Vec<PendingDelete> = serde_json::from_str(&content).unwrap_or_default();
    
    Ok(pending)
}

/// Delete a folder (files inside are moved back to root)
#[tauri::command]
pub async fn delete_folder(folder_id: String) -> Result<(), String> {
    // First, unassign all files from this folder
    let metadata_dir = get_metadata_dir()?;
    let entries = fs::read_dir(&metadata_dir)
        .map_err(|e| format!("Failed to read metadata dir: {}", e))?;

    for entry in entries {
        let entry = entry.map_err(|e| format!("Failed to read entry: {}", e))?;
        let path = entry.path();

        if path.extension().map_or(false, |ext| ext == "json") {
            let content = fs::read_to_string(&path)
                .map_err(|e| format!("Failed to read file: {}", e))?;
            if let Ok(mut file_meta) = serde_json::from_str::<ChunkedFile>(&content) {
                if file_meta.folder_id.as_deref() == Some(&folder_id) {
                    file_meta.folder_id = None;
                    let json = serde_json::to_string_pretty(&file_meta)
                        .map_err(|e| format!("Failed to serialize: {}", e))?;
                    fs::write(&path, json)
                        .map_err(|e| format!("Failed to write: {}", e))?;
                }
            }
        }
    }

    // Delete the folder metadata
    let folders_dir = get_folders_dir()?;
    let folder_path = folders_dir.join(format!("{}.json", folder_id));
    if folder_path.exists() {
        fs::remove_file(&folder_path)
            .map_err(|e| format!("Failed to delete folder: {}", e))?;
    }

    println!("[Folders] ✓ Deleted folder: {}", folder_id);
    Ok(())
}

/// Move a file into a folder (or back to root if folder_id is None)
#[tauri::command]
pub async fn move_file_to_folder(file_id: String, folder_id: Option<String>) -> Result<(), String> {
    let metadata_dir = get_metadata_dir()?;
    let metadata_path = metadata_dir.join(format!("{}.json", file_id));

    if !metadata_path.exists() {
        return Err(format!("File not found: {}", file_id));
    }

    // If moving to a folder, verify it exists
    if let Some(ref fid) = folder_id {
        let folders_dir = get_folders_dir()?;
        let folder_path = folders_dir.join(format!("{}.json", fid));
        if !folder_path.exists() {
            return Err(format!("Folder not found: {}", fid));
        }
    }

    let content = fs::read_to_string(&metadata_path)
        .map_err(|e| format!("Failed to read file metadata: {}", e))?;
    let mut file_meta: ChunkedFile = serde_json::from_str(&content)
        .map_err(|e| format!("Failed to parse metadata: {}", e))?;

    file_meta.folder_id = folder_id.clone();

    let json = serde_json::to_string_pretty(&file_meta)
        .map_err(|e| format!("Failed to serialize: {}", e))?;
    fs::write(&metadata_path, json)
        .map_err(|e| format!("Failed to write metadata: {}", e))?;

    println!("[Folders] ✓ Moved file {} to folder {:?}", file_id, folder_id);
    Ok(())
}

/// List files in a specific folder (or root if folder_id is None)
#[tauri::command]
pub async fn list_files_in_folder(folder_id: Option<String>) -> Result<Vec<ChunkedFile>, String> {
    let all_files = list_chunked_files().await?;
    let filtered: Vec<ChunkedFile> = all_files
        .into_iter()
        .filter(|f| f.folder_id == folder_id)
        .collect();
    Ok(filtered)
}